{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd5f59be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from scipy.stats import pearsonr\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103f6618",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_csv(\"pittsburghArrest.csv\")\n",
    "\n",
    "df = df[[\"AGE\", \"INCIDENTNEIGHBORHOOD\"]]\n",
    "df = df.dropna(subset=[\"AGE\", \"INCIDENTNEIGHBORHOOD\"])\n",
    "df[\"AGE\"] = pd.to_numeric(df[\"AGE\"], errors=\"coerce\")\n",
    "df = df.dropna(subset=[\"AGE\"])\n",
    "\n",
    "df[\"AGE_GROUP\"] = df[\"AGE\"].apply(lambda x: \"<18\" if x < 18 else \"18+\")\n",
    "\n",
    "grouped = df.groupby([\"INCIDENTNEIGHBORHOOD\", \"AGE_GROUP\"]).size().unstack(fill_value=0)\n",
    "\n",
    "grouped[\"Total_Count\"] = grouped[\"<18\"] + grouped[\"18+\"]\n",
    "grouped[\"Under_18_Percent_Local\"] = grouped[\"<18\"] / grouped[\"Total_Count\"]\n",
    "grouped[\"Over_18_Percent_Local\"] = grouped[\"18+\"] / grouped[\"Total_Count\"]\n",
    "\n",
    "total_under_18 = grouped[\"<18\"].sum()\n",
    "total_over_18 = grouped[\"18+\"].sum()\n",
    "\n",
    "grouped[\"Under_18_Percent_Global\"] = grouped[\"<18\"] / total_under_18\n",
    "grouped[\"Over_18_Percent_Global\"] = grouped[\"18+\"] / total_over_18\n",
    "\n",
    "result = grouped.reset_index().rename(columns={\n",
    "    \"INCIDENTNEIGHBORHOOD\": \"NEIGHBORHOOD\",\n",
    "    \"<18\": \"Under_18_Count\",\n",
    "    \"18+\": \"Over_18_Count\"\n",
    "})\n",
    "\n",
    "result = result[\n",
    "    [\"NEIGHBORHOOD\", \"Under_18_Count\", \"Over_18_Count\", \"Total_Count\",\n",
    "     \"Under_18_Percent_Local\", \"Over_18_Percent_Local\",\n",
    "     \"Under_18_Percent_Global\", \"Over_18_Percent_Global\"]\n",
    "]\n",
    "\n",
    "result.to_csv(\"summaryPittArrest.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebc2af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"pittsburghParks.csv\")\n",
    "\n",
    "\n",
    "df = df[[\"type\", \"neighborhood\"]]\n",
    "df = df.dropna(subset=[\"type\", \"neighborhood\"])\n",
    "\n",
    "\n",
    "df[\"IS_PARK\"] = df[\"type\"].apply(lambda x: \"Park\" if x.strip().lower() == \"park\" else \"Other\")\n",
    "\n",
    "grouped = df.groupby([\"neighborhood\", \"IS_PARK\"]).size().unstack(fill_value=0)\n",
    "\n",
    "if \"Park\" not in grouped.columns:\n",
    "    grouped[\"Park\"] = 0\n",
    "if \"Other\" not in grouped.columns:\n",
    "    grouped[\"Other\"] = 0\n",
    "\n",
    "total_park = grouped[\"Park\"].sum()\n",
    "total_other = grouped[\"Other\"].sum()\n",
    "\n",
    "grouped[\"Park_Global_Percent\"] = grouped[\"Park\"] / total_park\n",
    "grouped[\"Other_Global_Percent\"] = grouped[\"Other\"] / total_other\n",
    "\n",
    "result = grouped.reset_index().rename(columns={\n",
    "    \"neighborhood\": \"NEIGHBORHOOD\",\n",
    "    \"Park\": \"Park_Count\",\n",
    "    \"Other\": \"Non_Park_Count\"\n",
    "})\n",
    "\n",
    "result = result[\n",
    "    [\"NEIGHBORHOOD\", \"Park_Count\", \"Non_Park_Count\", \"Park_Global_Percent\", \"Other_Global_Percent\"]\n",
    "]\n",
    "\n",
    "result.to_csv(\"summaryPittPark.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e20ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_csv(\"pittsburghFacility.csv\")\n",
    "\n",
    "df = df[[\"type\", \"neighborhood\"]]\n",
    "df = df.dropna(subset=[\"type\", \"neighborhood\"])\n",
    "\n",
    "all_types = df[\"type\"].unique()\n",
    "\n",
    "pivot_table = df.groupby([\"neighborhood\", \"type\"]).size().unstack(fill_value=0)\n",
    "\n",
    "type_totals = pivot_table.sum()\n",
    "\n",
    "type_percents = pivot_table.divide(type_totals)\n",
    "\n",
    "final_df = pivot_table.copy()\n",
    "for col in pivot_table.columns:\n",
    "    final_df[f\"{col}_Count\"] = pivot_table[col]\n",
    "    final_df[f\"{col}_Percent\"] = type_percents[col]\n",
    "    final_df.drop(columns=[col], inplace=True)\n",
    "    \n",
    "final_df[\"Total_Facilities\"] = final_df[[col for col in final_df.columns if col.endswith(\"_Count\")]].sum(axis=1)\n",
    "final_df = final_df.reset_index().rename(columns={\"neighborhood\": \"NEIGHBORHOOD\"})\n",
    "\n",
    "count_cols = [col for col in final_df.columns if col.endswith(\"_Count\")]\n",
    "percent_cols = [col for col in final_df.columns if col.endswith(\"_Percent\")]\n",
    "final_df = final_df[[\"NEIGHBORHOOD\", \"Total_Facilities\"] + count_cols + percent_cols]\n",
    "\n",
    "final_df.to_csv(\"summaryPittFacilities.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe08f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_csv(\"pittsburghSteps.csv\")\n",
    "\n",
    "df = df[[\"neighborhood\", \"length\", \"number_of_steps\"]]\n",
    "df = df.dropna(subset=[\"neighborhood\", \"length\", \"number_of_steps\"])\n",
    "\n",
    "df_with_steps = df[df[\"number_of_steps\"] > 0]\n",
    "df_no_steps = df[df[\"number_of_steps\"] == 0]\n",
    "\n",
    "grouped_steps = df_with_steps.groupby(\"neighborhood\").agg({\n",
    "    \"length\": \"sum\",\n",
    "    \"number_of_steps\": \"sum\"\n",
    "}).rename(columns={\n",
    "    \"length\": \"Total_Length_With_Steps\",\n",
    "    \"number_of_steps\": \"Total_Steps\"\n",
    "})\n",
    "\n",
    "grouped_steps[\"Length_per_Step\"] = grouped_steps[\"Total_Length_With_Steps\"] / grouped_steps[\"Total_Steps\"]\n",
    "\n",
    "grouped_no_steps = df_no_steps.groupby(\"neighborhood\").agg({\n",
    "    \"length\": \"sum\"\n",
    "}).rename(columns={\"length\": \"Zero_Steps_Total_Length\"})\n",
    "\n",
    "final_df = grouped_steps.join(grouped_no_steps, how=\"outer\").fillna(0)\n",
    "\n",
    "final_df = final_df.reset_index().rename(columns={\"neighborhood\": \"NEIGHBORHOOD\"})\n",
    "\n",
    "final_df.to_csv(\"summaryPittSteps.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d052412d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"pittsburghTrees.csv\")\n",
    "\n",
    "tree_count = df.groupby(\"neighborhood\").size().reset_index(name=\"tree_count\")\n",
    "\n",
    "tree_count.to_csv(\"summaryPittTrees.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01a8e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "gdf = gpd.read_file(\"../GeopandasTest/hood.geojson\")\n",
    "# print(gdf[\"hood\"].unique())\n",
    "# print(gdf[\"Shape__Area\"].head())\n",
    "print(gdf.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e6bda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# arrest_df = pd.read_csv(\"summaryPittArrest.csv\")\n",
    "# park_df = pd.read_csv(\"summaryPittPark.csv\")\n",
    "# fac_df = pd.read_csv(\"summaryPittFacilities.csv\")\n",
    "# step_df = pd.read_csv(\"summaryPittSteps.csv\")\n",
    "\n",
    "# arrest_neigh = set(arrest_df[\"NEIGHBORHOOD\"])\n",
    "# fac_neigh = set(fac_df[\"NEIGHBORHOOD\"])\n",
    "# step_neigh = set(step_df[\"NEIGHBORHOOD\"])\n",
    "# park_neigh = set(park_df[\"NEIGHBORHOOD\"])\n",
    "\n",
    "# core_common = arrest_neigh & fac_neigh & step_neigh\n",
    "\n",
    "# all_core = arrest_neigh | fac_neigh | step_neigh\n",
    "# missing_core = all_core - core_common\n",
    "# if missing_core:\n",
    "#     print(\"The following neighborhood is missing in the dataset(Arrest/Facilities/Steps), and will not be counted:\")\n",
    "#     for name in sorted(missing_core):\n",
    "#         print(\"-\", name)\n",
    "\n",
    "# arrest_df = arrest_df[arrest_df[\"NEIGHBORHOOD\"].isin(core_common)]\n",
    "# fac_df = fac_df[fac_df[\"NEIGHBORHOOD\"].isin(core_common)]\n",
    "# step_df = step_df[step_df[\"NEIGHBORHOOD\"].isin(core_common)]\n",
    "\n",
    "# park_df = park_df[park_df[\"NEIGHBORHOOD\"].isin(core_common)]\n",
    "\n",
    "# merged = arrest_df.merge(fac_df, on=\"NEIGHBORHOOD\")\n",
    "# merged = merged.merge(step_df, on=\"NEIGHBORHOOD\")\n",
    "# merged = merged.merge(park_df, on=\"NEIGHBORHOOD\", how=\"left\")\n",
    "\n",
    "# merged = merged.fillna(0)\n",
    "\n",
    "# merged.to_csv(\"summaryPitt_ALL_MERGED.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55941500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ ç¼ºå¤±çš„è¡—åŒºï¼ˆUnder_18_Count æ•°æ®ç¼ºå¤±ï¼‰ï¼š {'Mt. Oliver', 'Mount Oliver Borough'}\n",
      "âš ï¸ ç¼ºå¤±çš„è¡—åŒºï¼ˆTotal_Facilities æ•°æ®ç¼ºå¤±ï¼‰ï¼š {'Mt. Oliver', 'Troy Hill-Herrs Island', 'Northview Heights', 'Outside State', 'Spring Garden', 'Outside County', 'Esplen', 'Outside City', 'Summer Hill', 'Arlington', 'Central North Side', 'Golden Triangle/Civic Arena', 'Friendship', 'Chartiers City', 'North Shore', 'Mt. Oliver Boro', 'Mt. Oliver Neighborhood', 'Mount Oliver Borough', 'New Homestead', 'East Carnegie', 'South Shore', 'Ridgemont', 'St. Clair', 'Mount Oliver', 'Arlington Heights'}\n",
      "âš ï¸ ç¼ºå¤±çš„è¡—åŒºï¼ˆTotal_Length_With_Steps æ•°æ®ç¼ºå¤±ï¼‰ï¼š {'Swisshelm Park', 'Troy Hill-Herrs Island', 'Northview Heights', 'Outside State', 'Outside County', 'Homewood South', 'Outside City', 'Summer Hill', 'Chateau', 'Central North Side', 'Golden Triangle/Civic Arena', 'Friendship', 'Mt. Oliver Boro', 'Mt. Oliver Neighborhood', 'Fairywood', 'Homewood West', 'New Homestead', 'South Shore', 'Allegheny West', 'Hays', 'Mount Oliver', 'Arlington Heights'}\n",
      "âš ï¸ ç¼ºå¤±çš„è¡—åŒºï¼ˆtree_count æ•°æ®ç¼ºå¤±ï¼‰ï¼š {'Outside State', 'Outside County', 'Outside City', 'Troy Hill-Herrs Island', 'Mt. Oliver Boro', 'Mt. Oliver Neighborhood', 'Mount Oliver', 'Central North Side', 'Golden Triangle/Civic Arena', 'Mount Oliver Borough'}\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'json' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 33\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# --- è¯»å– geojson é¢ç§¯ ---\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhood.geojson\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m---> 33\u001b[0m     geo \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m     35\u001b[0m area_data \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m feat \u001b[38;5;129;01min\u001b[39;00m geo[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'json' is not defined"
     ]
    }
   ],
   "source": [
    "# --- è¯»å–æ‰€æœ‰æ•°æ® ---\n",
    "df_arrest = pd.read_csv(\"summaryPittArrest.csv\")\n",
    "df_park = pd.read_csv(\"summaryPittPark.csv\")\n",
    "df_facilities = pd.read_csv(\"summaryPittFacilities.csv\")\n",
    "df_steps = pd.read_csv(\"summaryPittSteps.csv\")\n",
    "df_trees = pd.read_csv(\"summaryPittTrees.csv\")\n",
    "\n",
    "# --- åˆå¹¶å‰ç»Ÿä¸€åˆ—åä¿æŒä¸€è‡´ ---\n",
    "for df in [df_arrest, df_park, df_facilities, df_steps, df_trees]:\n",
    "    df.columns = [col.strip() for col in df.columns]\n",
    "\n",
    "# --- åˆå¹¶ ---\n",
    "dfs = [df_arrest, df_facilities, df_steps, df_trees]\n",
    "merged = df_park.copy()\n",
    "\n",
    "for df in dfs:\n",
    "    merged = pd.merge(merged, df, on=\"NEIGHBORHOOD\", how=\"outer\")\n",
    "\n",
    "# --- ç¼ºå¤±å¡« 0ï¼ˆé™¤äº† park ä¹‹å¤–çš„ç¼ºå¤±æ‰“å°å‡ºæ¥ï¼‰ ---\n",
    "park_neighborhoods = set(df_park[\"NEIGHBORHOOD\"])\n",
    "all_neighborhoods = set(merged[\"NEIGHBORHOOD\"])\n",
    "\n",
    "for df in [df_arrest, df_facilities, df_steps, df_trees]:\n",
    "    current_hoods = set(df[\"NEIGHBORHOOD\"])\n",
    "    missing = all_neighborhoods - current_hoods\n",
    "    if df is not df_park:\n",
    "        print(f\"âš ï¸ ç¼ºå¤±çš„è¡—åŒºï¼ˆ{df.columns[1]} æ•°æ®ç¼ºå¤±ï¼‰ï¼š\", missing)\n",
    "\n",
    "merged.fillna(0, inplace=True)\n",
    "\n",
    "# --- è¯»å– geojson é¢ç§¯ ---\n",
    "with open(\"hood.geojson\", \"r\", encoding=\"utf-8\") as f:\n",
    "    geo = geo.load(f)\n",
    "\n",
    "area_data = []\n",
    "for feat in geo[\"features\"]:\n",
    "    name = feat[\"properties\"][\"hood\"]\n",
    "    area = feat[\"properties\"][\"Shape__Area\"]\n",
    "    area_data.append({\"NEIGHBORHOOD\": name, \"Shape__Area\": area})\n",
    "\n",
    "df_area = pd.DataFrame(area_data)\n",
    "\n",
    "# --- åˆå¹¶é¢ç§¯æ•°æ® ---\n",
    "merged = pd.merge(merged, df_area, on=\"NEIGHBORHOOD\", how=\"left\")\n",
    "\n",
    "# --- é¢ç§¯å½’ä¸€åŒ– ---\n",
    "area_min = merged[\"Shape__Area\"].min()\n",
    "area_max = merged[\"Shape__Area\"].max()\n",
    "merged[\"normalized_area\"] = (merged[\"Shape__Area\"] - area_min) / (area_max - area_min)\n",
    "\n",
    "# --- æ‰¾å‡ºéœ€è¦åšå•ä½åŒ–çš„åˆ—ï¼ˆæ’é™¤æ‰å·²æœ‰çš„_unitåˆ—å’Œé¢ç§¯ç›¸å…³åˆ—ï¼‰ ---\n",
    "exclude_keywords = [\"_unit\", \"Shape__Area\", \"normalized_area\", \"NEIGHBORHOOD\"]\n",
    "original_cols = [col for col in merged.columns if all(key not in col for key in exclude_keywords)]\n",
    "\n",
    "# --- ä¸€æ¬¡æ€§ç”Ÿæˆæ‰€æœ‰å•ä½åˆ—ï¼Œé¿å…æ€§èƒ½è­¦å‘Š ---\n",
    "unit_data = {}\n",
    "for col in original_cols:\n",
    "    unit_col = col + \"_unit\"\n",
    "    # é¿å…é™¤ä»¥ 0\n",
    "    unit_data[unit_col] = merged[col] / merged[\"normalized_area\"].replace(0, 1e-6)\n",
    "\n",
    "unit_df = pd.DataFrame(unit_data)\n",
    "\n",
    "# --- åˆå¹¶å•ä½åˆ—åˆ°æœ€å‰é¢ ---\n",
    "merged = pd.concat([merged[[\"NEIGHBORHOOD\"]], unit_df, merged.drop(columns=[\"NEIGHBORHOOD\"])], axis=1)\n",
    "\n",
    "# --- å¯¼å‡º ---\n",
    "merged.to_csv(\"summaryPitt_ALL_MERGED_UNIT.csv\", index=False)\n",
    "print(\"âœ… å…¨éƒ¨æå®šï¼æœ€ç»ˆæ–‡ä»¶ä¿å­˜ä¸ºï¼šsummaryPitt_ALL_MERGED_UNIT.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a736a0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"summaryPitt_ALL_MERGED_UNIT.csv\")\n",
    "\n",
    "columns_to_analyze = [\n",
    "    \"Under_18_Crime_Count_unit\", \"Over_18_Crime_Count_unit\",\n",
    "    \"Total_Crime_Count_unit\",\n",
    "    \"Park_Count_unit\",\n",
    "    # \"Park\", \"Not_Park\", \"Park_Global_Percent\", \"Not_Park_Global_Percent\",\n",
    "\n",
    "    \"Total_Facilities_unit\",\n",
    "\n",
    "    \"Total_Length_With_Steps_unit\", \"Total_Steps_unit\", \"Length_per_Step_unit\"\n",
    "    # , \"Zero_Steps_Total_Length\",\n",
    "]\n",
    "\n",
    "correlation_matrix = df[columns_to_analyze].corr()\n",
    "\n",
    "plt.figure(figsize=(16, 12))\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt=\".2f\", cmap=\"coolwarm\", square=True,\n",
    "            linewidths=.5, cbar_kws={\"shrink\": .8})\n",
    "\n",
    "plt.title(\"Correlation Matrix\", fontsize=18)\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(\"correlation_matrix.png\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1666d7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df[columns_to_analyze].copy()\n",
    "\n",
    "# --- å®šä¹‰ä¸€ä¸ªå¸¦ r å€¼çš„å›å½’å›¾å‡½æ•° ---\n",
    "def regplot_with_r(x, y, **kwargs):\n",
    "    ax = kwargs.get(\"ax\", plt.gca())\n",
    "    sns.regplot(x=x, y=y, ax=ax,\n",
    "                scatter_kws={\"s\": 20, \"alpha\": 0.6},\n",
    "                line_kws={\"color\": \"red\"})\n",
    "    # è®¡ç®— r å€¼\n",
    "    r, _ = pearsonr(x, y)\n",
    "    ax.annotate(f\"r = {r:.2f}\", xy=(0.05, 0.9), xycoords='axes fraction',\n",
    "                fontsize=11, color='black')\n",
    "\n",
    "# --- åˆ›å»º PairGrid ---\n",
    "g = sns.PairGrid(data, height=2.5)\n",
    "\n",
    "# å·¦ä¸‹è§’å’Œå³ä¸Šè§’éƒ½ç”¨ regplot_with_r\n",
    "g.map_lower(regplot_with_r)\n",
    "g.map_upper(regplot_with_r)\n",
    "\n",
    "# å¯¹è§’çº¿ç”»åˆ†å¸ƒå›¾\n",
    "g.map_diag(sns.histplot, kde=True)\n",
    "\n",
    "# æ ‡é¢˜ & ç¾åŒ–\n",
    "plt.suptitle(\"ğŸ“Š æ‰€æœ‰å˜é‡ä¸¤ä¸¤çº¿æ€§æ‹Ÿåˆ + Pearson r å€¼\", fontsize=16, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=0.95)  # ç»™æ ‡é¢˜ç•™ç©ºé—´\n",
    "plt.savefig(\"pairwise_full_regplot_r.png\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6bf819b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "INFSCI0510",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
